#!/usr/bin/env python3
"""
Enhanced Auto-Editor - Smart Video Editor

Removes silence, enhances audio (DeepFilterNet AI), and removes filler words.

3-PHASE PIPELINE:
  Phase 1: Cut silent parts (auto-editor, fast, local)
  Phase 2: Enhance audio with DeepFilterNet AI (studio-quality voice)
  Phase 3: Cut awkward parts / fillers (transcribe ‚Üí detect ‚Üí cut)

Supports:
  - Local Whisper (small model) for 4GB VRAM GPUs
  - Groq API (Whisper Large V3 Turbo) for better accuracy (FREE tier)
  - DeepFilterNet AI for professional voice enhancement

Usage:
  edit video.mp4                    # Interactive mode
  edit video.mp4 --local            # Force local Whisper small
  edit video.mp4 --groq             # Force Groq API (recommended)
  edit video.mp4 --set-groq-key     # Set/update Groq API key
  edit video.mp4 --no-enhance       # Skip audio enhancement
  edit video.mp4 --render           # Render final video (default: XML only)

Groq Free Tier Limits:
  - 7,200 audio seconds/hour (2 hours of audio per hour)
  - 28,800 audio seconds/day (8 hours of audio per day)
  - By removing silence FIRST, you use less API quota!
"""

import os
import sys
import subprocess
import json
import math
import argparse
import time
import tempfile
import shutil
import signal
from pathlib import Path
from dataclasses import dataclass
from typing import Optional, List, Tuple

# --- Environment Cleanup ---
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# --- Hyprwhspr Management ---
HYPRWHSPR_COMMAND = []

def manage_hyprwhspr(stop=True):
    """Detects and kills/restarts hyprwhspr to free up VRAM"""
    global HYPRWHSPR_COMMAND
    try:
        if stop:
            # Find hyprwhspr process command line
            result = subprocess.run(["pgrep", "-f", "hyprwhspr"], capture_output=True, text=True)
            pids = [p for p in result.stdout.strip().split("\n") if p]
            
            if pids:
                # Get the command line of the first one to restart it later
                cmd_result = subprocess.run(["ps", "-p", pids[0], "-o", "args="], capture_output=True, text=True)
                full_cmd = cmd_result.stdout.strip()
                if full_cmd:
                    HYPRWHSPR_COMMAND = full_cmd.split()
                
                info(f"Detected hyprwhspr (using 1.9GB VRAM). Killing to free memory...")
                subprocess.run(["pkill", "-f", "hyprwhspr"])
                time.sleep(1) # Wait for VRAM to clear
        else:
            # restart
            if HYPRWHSPR_COMMAND:
                info(f"Restarting hyprwhspr: {' '.join(HYPRWHSPR_COMMAND[:3])}...")
                subprocess.Popen(HYPRWHSPR_COMMAND, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    except Exception as e:
        warn(f"Hyprwhspr management failed: {e}")

# --- Fix for missing CUDA libraries (Arch Linux / venv) ---
def add_cuda_to_path():
    """Adds local nvidia pip package paths to LD_LIBRARY_PATH and re-execs if needed"""
    import site
    
    # 1. Identify all possible site-packages
    search_paths = [Path(site.getusersitepackages())]
    if hasattr(sys, 'real_prefix') or (sys.base_prefix != sys.prefix):
        # We are in a venv
        venv_site = Path(sys.prefix) / "lib" / f"python{sys.version_info.major}.{sys.version_info.minor}" / "site-packages"
        search_paths.append(venv_site)
    
    # 2. Find nvidia library directories
    cuda_libs = []
    for sp in search_paths:
        if not sp.exists(): continue
        # Check all nvidia subpackages
        nvidia_dir = sp / "nvidia"
        if nvidia_dir.exists():
            for sub in nvidia_dir.iterdir():
                lib_dir = sub / "lib"
                if lib_dir.exists():
                    cuda_libs.append(str(lib_dir))
    
    if not cuda_libs:
        return

    # 3. Check if they are already in LD_LIBRARY_PATH
    current_ld = os.environ.get("LD_LIBRARY_PATH", "")
    missing = [p for p in cuda_libs if p not in current_ld]
    
    if missing:
        # We need to update LD_LIBRARY_PATH and RE-EXECUTE the script
        # because LD_LIBRARY_PATH is only read at process startup
        new_ld = ":".join(cuda_libs + ([current_ld] if current_ld else []))
        os.environ["LD_LIBRARY_PATH"] = new_ld
        
        # Prevent infinite loop
        if os.environ.get("EDIT_REEXEC") == "1":
            return
        os.environ["EDIT_REEXEC"] = "1"
        
        # Restart the script with the new environment
        os.execv(sys.executable, [sys.executable] + sys.argv)

add_cuda_to_path()

# --- Configuration ---
CONFIG_DIR = Path.home() / ".config" / "enhanced-auto-editor"
CONFIG_FILE = CONFIG_DIR / "config.json"
DEEP_FILTER_BIN = Path.home() / ".local" / "bin" / "deep-filter"

# Filler words in Arabic (Egyptian/Levantine dialect) and English
FILLERS = {
    # English fillers
    "um", "uh", "ah", "so", "like", "emm", "mmm", "umm", "uhh", "ahh", "aa", "aaa",
    "hmm", "hm", "er", "erm", "eh", "mhm", "ugh", "well", "okay", "ok",
    # Arabic fillers (Egyptian/Levantine/Gulf)
    "yani", "ya'ni", "y'ni", "yaani", "tayyib", "tayeb", "bass", "bas",
    "ŸäÿπŸÜŸä", "ÿßŸÖŸÖ", "ÿßÿß", "ÿßŸá", "ÿ¢Ÿá", "ÿßŸáŸáŸá", "ÿßŸÖŸÖŸÖŸÖ",
    "ÿ∑ÿ®", "ÿ®ŸÇŸâ", "ŸÖÿßÿ¥Ÿä", "ÿ™ŸÖÿßŸÖ", "ÿßÿµŸÑ", "ŸÅ", "ÿ®ÿ≥", "ŸäŸÑÿß",
    "ŸáŸÖ", "ÿßŸáŸÖ", "ÿßŸäŸàŸá", "ÿßŸàŸÉŸä", "ÿÆŸÑÿßÿµ", "ŸÉÿØŸá", "ÿ≤Ÿä", "ÿπÿßÿ±ŸÅ",
    "ŸàÿßŸÑŸÑŸá", "ÿßŸä", "ÿ¢ÿ¢", "ÿßÿßÿß", "ÿßÿßÿßÿß",
}

# Detection thresholds
MIN_CONFIDENCE = 0.35
FILLER_MIN_DURATION = 0.08
FILLER_MAX_DURATION = 1.5

# Groq API settings
GROQ_CHUNK_SIZE = 300
GROQ_OVERLAP = 1.5
GROQ_MODEL = "whisper-large-v3-turbo"

# --- Color Output ---
class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BOLD = '\033[1m'
    END = '\033[0m'

def cprint(msg, color=Colors.END):
    print(f"{color}{msg}{Colors.END}")

def success(msg): cprint(f"‚úì {msg}", Colors.GREEN)
def info(msg): cprint(f"‚Üí {msg}", Colors.CYAN)
def warn(msg): cprint(f"‚ö† {msg}", Colors.YELLOW)
def error(msg): cprint(f"‚úó {msg}", Colors.RED)

# --- Progress Bar ---
class ProgressBar:
    def __init__(self, total=100, width=40):
        self.total = max(float(total), 0.01)
        self.width = width
        self.spinner = ["‚†ã", "‚†ô", "‚†π", "‚†∏", "‚†º", "‚†¥", "‚†¶", "‚†ß", "‚†á", "‚†è"]
        self.idx = 0

    def update(self, current, desc="Processing"):
        pct = min(100, max(0, (current / self.total) * 100))
        filled = int(self.width * current / self.total)
        bar = "‚ñà" * filled + "‚ñë" * (self.width - filled)
        spin = self.spinner[self.idx % len(self.spinner)]
        self.idx += 1
        sys.stdout.write(f"\r{spin} {desc:<25} |{Colors.CYAN}{bar}{Colors.END}| {pct:5.1f}%")
        sys.stdout.flush()

    def finish(self, msg="Done"):
        bar = "‚ñà" * self.width
        sys.stdout.write(f"\r‚úì {msg:<25} |{Colors.GREEN}{bar}{Colors.END}| 100.0%\n")
        sys.stdout.flush()

# --- Configuration Management ---
def load_config() -> dict:
    if CONFIG_FILE.exists():
        try:
            return json.loads(CONFIG_FILE.read_text())
        except:
            pass
    return {}

def save_config(cfg: dict):
    CONFIG_DIR.mkdir(parents=True, exist_ok=True)
    CONFIG_FILE.write_text(json.dumps(cfg, indent=2))

def get_groq_key() -> Optional[str]:
    key = os.environ.get("GROQ_API_KEY")
    if key:
        return key
    cfg = load_config()
    return cfg.get("groq_api_key")

def set_groq_key(key: str):
    cfg = load_config()
    cfg["groq_api_key"] = key
    save_config(cfg)
    success("Groq API key saved!")

# --- FFmpeg Utilities ---
def get_duration(file_path: str) -> float:
    cmd = [
        "ffprobe", "-v", "error", "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1", str(file_path)
    ]
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        return float(result.stdout.strip())
    except:
        return 0.0

def format_duration(seconds: float) -> str:
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    if h > 0:
        return f"{h}h {m}m {s}s"
    elif m > 0:
        return f"{m}m {s}s"
    return f"{s}s"

def find_auto_editor() -> str:
    ae_path = shutil.which("auto-editor")
    if ae_path:
        return ae_path
    for p in [
        Path.home() / "venv" / "bin" / "auto-editor",
        Path("/usr/bin/auto-editor"),
        Path("/usr/local/bin/auto-editor"),
    ]:
        if p.exists():
            return str(p)
    error("auto-editor not found! Install it: pip install auto-editor")
    sys.exit(1)

# --- DeepFilterNet Audio Enhancement ---
def enhance_audio_deepfilter(input_path: Path, output_path: Path) -> bool:
    """
    Phase 2: Enhance audio using DeepFilterNet AI.
    
    DeepFilterNet is a state-of-the-art deep learning model for:
    - Background noise removal
    - Voice enhancement
    - Echo reduction
    
    Produces clean, studio-quality voice without EQ/compression artifacts.
    """
    if not DEEP_FILTER_BIN.exists():
        warn(f"DeepFilterNet not found at {DEEP_FILTER_BIN}")
        warn("Install: curl -L -o ~/.local/bin/deep-filter 'https://github.com/Rikorose/DeepFilterNet/releases/download/v0.5.6/deep-filter-0.5.6-x86_64-unknown-linux-musl' && chmod +x ~/.local/bin/deep-filter")
        return False
    
    info("Running DeepFilterNet AI enhancement...")
    
    with tempfile.TemporaryDirectory() as tmpdir:
        tmpdir = Path(tmpdir)
        
        # Extract audio as WAV (48kHz for DeepFilterNet)
        wav_input = tmpdir / "input.wav"
        info("  Extracting audio...")
        cmd_extract = [
            "ffmpeg", "-y", "-hide_banner", "-loglevel", "error",
            "-i", str(input_path),
            "-vn", "-ar", "48000", "-ac", "1",
            str(wav_input)
        ]
        try:
            subprocess.run(cmd_extract, check=True, capture_output=True)
        except subprocess.CalledProcessError as e:
            warn(f"Audio extraction failed: {e}")
            return False
        
        # Run DeepFilterNet
        info("  Processing with AI...")
        out_dir = tmpdir / "out"
        cmd_df = [
            str(DEEP_FILTER_BIN),
            "--compensate-delay",
            "-o", str(out_dir),
            str(wav_input)
        ]
        try:
            subprocess.run(cmd_df, check=True, capture_output=True)
        except subprocess.CalledProcessError as e:
            warn(f"DeepFilterNet failed: {e}")
            return False
        
        # Find the output file
        enhanced_wav = out_dir / "input.wav"
        if not enhanced_wav.exists():
            warn("DeepFilterNet output not found")
            return False
        
        # Merge enhanced audio with video + loudness normalization
        info("  Merging enhanced audio...")
        cmd_merge = [
            "ffmpeg", "-y", "-hide_banner", "-loglevel", "error",
            "-i", str(input_path),
            "-i", str(enhanced_wav),
            "-map", "0:v",
            "-map", "1:a",
            "-c:v", "copy",
            "-af", "loudnorm=I=-16:TP=-1.5:LRA=11",
            "-c:a", "aac", "-b:a", "192k",
            str(output_path)
        ]
        try:
            subprocess.run(cmd_merge, check=True, capture_output=True)
            success("Audio enhancement complete!")
            return True
        except subprocess.CalledProcessError as e:
            warn(f"Audio merge failed: {e}")
            return False

# --- Whisper Transcription ---
@dataclass
class WordSegment:
    word: str
    start: float
    end: float
    confidence: float

def extract_audio_chunk(input_path: Path, output_path: Path, start: float, duration: float):
    cmd = [
        "ffmpeg", "-y", "-hide_banner", "-loglevel", "error",
        "-ss", str(start),
        "-i", str(input_path),
        "-t", str(duration),
        "-vn", "-ar", "16000", "-ac", "1",
        "-c:a", "flac",
        str(output_path)
    ]
    subprocess.run(cmd, check=True, capture_output=True)

def extract_audio_wav(input_path: Path, output_path: Path):
    cmd = [
        "ffmpeg", "-y", "-hide_banner", "-loglevel", "error",
        "-i", str(input_path),
        "-vn", "-ar", "16000", "-ac", "1",
        "-c:a", "pcm_s16le",
        str(output_path)
    ]
    subprocess.run(cmd, check=True, capture_output=True)

def transcribe_local(audio_path: Path, duration: float) -> List[WordSegment]:
    """Transcribe with GPU-only fallback: large-v3-turbo ‚Üí large-v3 ‚Üí medium ‚Üí small ‚Üí base ‚Üí tiny"""
    try:
        from faster_whisper import WhisperModel
    except ImportError:
        error("faster-whisper not installed! Run: pip install faster-whisper")
        sys.exit(1)
    
    prompt = "ŸäÿπŸÜŸä, ÿßŸÖŸÖ, ÿßÿß, ÿßŸá, ÿ∑ÿ®, ÿ®ŸÇŸâ, Um, uh, ah, like, so, you know."
    
    # GPU-only fallback sequence
    # Note: large-v3-turbo might fail on 4GB VRAM, so we fall back to 'medium' (guaranteed to fit)
    models_to_try = [
        "deepdml/faster-whisper-large-v3-turbo-ct2", # large-v3-turbo (Best)
        "medium",                                      # medium (Reliable fallback for 4GB VRAM)
        "small",
        "base",
        "tiny"
    ]
    
    for model_name in models_to_try:
        display_name = model_name.split("/")[-1] # Cleaner name for display
        try:
            info(f"  Loading {display_name} model on GPU...")
            model = WhisperModel(model_name, device="cuda", compute_type="int8")
            success(f"  Loaded {display_name} on GPU")
            
            info("  Transcribing...")
            bar = ProgressBar(total=duration)
            
            segments_gen, _ = model.transcribe(
                str(audio_path),
                beam_size=5, # Slightly better quality for larger models
                word_timestamps=True,
                initial_prompt=prompt,
                language=None,
                vad_filter=True,
                vad_parameters=dict(min_silence_duration_ms=500)
            )
            
            words = []
            for segment in segments_gen:
                bar.update(segment.end, "Transcribing")
                if segment.words:
                    for w in segment.words:
                        words.append(WordSegment(
                            word=w.word.strip(),
                            start=w.start,
                            end=w.end,
                            confidence=math.exp(segment.avg_logprob)
                        ))
            
            bar.finish("Transcription complete")
            return words
            
        except Exception as e:
            warn(f"  {display_name} failed: {str(e)[:100]}...")
            if model_name != models_to_try[-1]:
                info(f"  Trying smaller model...")
            continue
    
    error("All GPU models failed! Try using --groq instead.")
    sys.exit(1)

def transcribe_groq_chunk(audio_path: Path, api_key: str, offset: float = 0) -> List[WordSegment]:
    try:
        from groq import Groq
    except ImportError:
        error("groq SDK not installed! Run: pip install groq")
        sys.exit(1)
    
    client = Groq(api_key=api_key)
    
    with open(audio_path, "rb") as f:
        response = client.audio.transcriptions.create(
            file=f,
            model=GROQ_MODEL,
            response_format="verbose_json",
            timestamp_granularities=["word"],
            language="ar",
            prompt="ŸäÿπŸÜŸä, ÿßŸÖŸÖ, ÿßÿß, ÿßŸá, ÿ∑ÿ®, ÿ®ŸÇŸâ, Um, uh, ah, like, so, you know."
        )
    
    words = []
    if hasattr(response, 'words') and response.words:
        for w in response.words:
            words.append(WordSegment(
                word=w.word if hasattr(w, 'word') else w.get('word', ''),
                start=(w.start if hasattr(w, 'start') else w.get('start', 0)) + offset,
                end=(w.end if hasattr(w, 'end') else w.get('end', 0)) + offset,
                confidence=0.9
            ))
    elif hasattr(response, 'segments') and response.segments:
        for seg in response.segments:
            if hasattr(seg, 'words') and seg.words:
                for w in seg.words:
                    words.append(WordSegment(
                        word=w.word if hasattr(w, 'word') else w.get('word', ''),
                        start=(w.start if hasattr(w, 'start') else w.get('start', 0)) + offset,
                        end=(w.end if hasattr(w, 'end') else w.get('end', 0)) + offset,
                        confidence=0.9
                    ))
    
    return words

def transcribe_groq(input_path: Path, duration: float, api_key: str, chunk_size: int) -> List[WordSegment]:
    info(f"  Using Groq API (Whisper Large V3 Turbo)...")
    
    chunks = []
    start = 0
    while start < duration:
        end = min(start + chunk_size, duration)
        chunks.append((start, end - start))
        start = end - GROQ_OVERLAP if end < duration else end
    
    info(f"  Processing {len(chunks)} chunks...")
    bar = ProgressBar(total=len(chunks))
    
    all_words = []
    
    with tempfile.TemporaryDirectory() as tmpdir:
        tmpdir = Path(tmpdir)
        
        for i, (start_time, chunk_duration) in enumerate(chunks):
            chunk_path = tmpdir / f"chunk_{i:04d}.flac"
            extract_audio_chunk(input_path, chunk_path, start_time, chunk_duration)
            
            size_mb = chunk_path.stat().st_size / (1024 * 1024)
            if size_mb > 24:
                warn(f"  Chunk {i} is {size_mb:.1f}MB (>24MB), may fail.")
            
            try:
                words = transcribe_groq_chunk(chunk_path, api_key, offset=start_time)
                all_words.extend(words)
            except Exception as e:
                warn(f"  Chunk {i} failed: {e}")
            
            bar.update(i + 1, f"Chunk {i+1}/{len(chunks)}")
            
            if i < len(chunks) - 1:
                time.sleep(0.5)
    
    bar.finish("Transcription complete")
    
    all_words.sort(key=lambda w: w.start)
    deduped = []
    for w in all_words:
        if not deduped or w.start >= deduped[-1].end - 0.1:
            deduped.append(w)
    
    return deduped

# --- Filler Detection ---
def detect_filler_cuts(words: List[WordSegment]) -> List[Tuple[float, float]]:
    cuts = []
    
    for w in words:
        clean = w.word.lower().strip().replace(",", "").replace(".", "").replace("?", "").replace("ÿå", "")
        duration = w.end - w.start
        
        if duration < FILLER_MIN_DURATION or duration > FILLER_MAX_DURATION:
            continue
        
        is_filler = False
        
        if clean in FILLERS:
            is_filler = True
        
        if len(clean) > 2 and len(set(clean)) == 1:
            is_filler = True
        
        if w.confidence < MIN_CONFIDENCE and duration < 1.0:
            is_filler = True
        
        if is_filler:
            cuts.append((max(0, w.start - 0.01), w.end + 0.01))
    
    return cuts

def merge_cuts(cuts: List[Tuple[float, float]], merge_gap: float = 0.1) -> List[Tuple[float, float]]:
    if not cuts:
        return []
    
    cuts = sorted(cuts, key=lambda x: x[0])
    merged = [cuts[0]]
    
    for start, end in cuts[1:]:
        last_start, last_end = merged[-1]
        if start <= last_end + merge_gap:
            merged[-1] = (last_start, max(last_end, end))
        else:
            merged.append((start, end))
    
    return merged

# === 3-PHASE PIPELINE ===

def phase1_cut_silence(input_path: Path, output_path: Path) -> Tuple[Path, float]:
    """
    PHASE 1: Cut silent parts
    Uses auto-editor for fast, local silence detection and removal.
    """
    info("Cutting silent parts...")
    
    ae_path = find_auto_editor()
    original_duration = get_duration(input_path)
    
    cmd = [
        ae_path,
        str(input_path),
        "--edit", "audio:threshold=3%,minclip=0.25s,mincut=0.15s",
        "--margin", "0.12s,0.18s",
        "--video-codec", "h264_nvenc",
        "--video-bitrate", "10M",
        "--audio-codec", "aac",
        "--audio-bitrate", "192k",
        "--output", str(output_path),
        "--progress", "machine",
    ]
    
    try:
        subprocess.run(cmd, capture_output=True, text=True, check=True)
        new_duration = get_duration(output_path)
        saved = original_duration - new_duration
        saved_pct = (saved / original_duration * 100) if original_duration > 0 else 0
        success(f"Silence cut: {format_duration(original_duration)} ‚Üí {format_duration(new_duration)} (-{saved_pct:.1f}%)")
        return output_path, new_duration
    except subprocess.CalledProcessError as e:
        error(f"auto-editor failed: {e.stderr[:500] if e.stderr else 'Unknown error'}")
        sys.exit(1)

def phase2_enhance_audio(input_path: Path, output_path: Path) -> Path:
    """
    PHASE 2: Enhance audio with DeepFilterNet AI
    Removes background noise, enhances voice clarity.
    """
    if enhance_audio_deepfilter(input_path, output_path):
        return output_path
    else:
        warn("DeepFilterNet failed, using original audio")
        return input_path

def phase3_cut_fillers(
    input_path: Path,
    duration: float,
    use_groq: bool,
    api_key: Optional[str],
    chunk_size: int,
    output_xml: Path,
    output_video: Optional[Path] = None,
    render: bool = False
) -> Tuple[List[WordSegment], Path, Optional[Path]]:
    """
    PHASE 3: Cut awkward parts (filler words)
    1. Transcribe audio to get word timestamps
    2. Detect filler words (um, ah, ŸäÿπŸÜŸä, etc.)
    3. Export Premiere Pro XML with cuts marked
    """
    
    # Step 1: Transcribe
    info("Transcribing audio...")
    if use_groq:
        if not api_key:
            error("No Groq API key!")
            sys.exit(1)
        audio_minutes = duration / 60
        info(f"  Audio: {format_duration(duration)} ({audio_minutes:.1f} min)")
        words = transcribe_groq(input_path, duration, api_key, chunk_size)
    else:
        temp_wav = input_path.parent / f"{input_path.stem}_temp.wav"
        try:
            info("  Extracting audio...")
            extract_audio_wav(input_path, temp_wav)
            words = transcribe_local(temp_wav, duration)
        finally:
            if temp_wav.exists():
                temp_wav.unlink()
    
    info(f"  Found {len(words)} words")
    
    # Step 2: Detect fillers
    info("Detecting filler words...")
    filler_cuts = detect_filler_cuts(words)
    merged_cuts = merge_cuts(filler_cuts)
    total_filler_time = sum(e - s for s, e in merged_cuts)
    info(f"  Found {len(merged_cuts)} fillers ({total_filler_time:.1f}s)")
    
    # Step 3: Export
    ae_path = find_auto_editor()
    
    cut_args = []
    if merged_cuts:
        ranges = [f"{s:.3f}s,{e:.3f}s" for s, e in merged_cuts]
        cut_args = ["--cut-out"] + ranges
    
    info("Exporting Premiere Pro XML...")
    cmd_xml = [
        ae_path,
        str(input_path),
        "--edit", "none",
    ] + cut_args + [
        "--export", "premiere",
        "--output", str(output_xml),
        "--progress", "machine",
    ]
    
    try:
        subprocess.run(cmd_xml, capture_output=True, text=True, check=True)
        success(f"XML exported: {output_xml.name}")
    except subprocess.CalledProcessError as e:
        warn(f"XML export failed: {e.stderr[:300] if e.stderr else 'Unknown'}")
    
    # Optional: Render final video
    rendered_path = None
    if render and output_video:
        info("Rendering final video...")
        cmd_render = [
            ae_path,
            str(input_path),
            "--edit", "none",
        ] + cut_args + [
            "--video-codec", "h264_nvenc",
            "--video-bitrate", "8M",
            "--audio-codec", "aac",
            "--audio-bitrate", "192k",
            "--output", str(output_video),
            "--progress", "machine",
        ]
        
        try:
            subprocess.run(cmd_render, capture_output=True, text=True, check=True)
            success(f"Video rendered: {output_video.name}")
            rendered_path = output_video
        except subprocess.CalledProcessError as e:
            warn(f"Render failed: {e.stderr[:300] if e.stderr else 'Unknown'}")
    
    return words, output_xml, rendered_path

# --- Main Processing Pipeline ---
def process_video(
    input_path: Path,
    use_groq: bool = False,
    chunk_size: int = GROQ_CHUNK_SIZE,
    enhance: bool = True,
    render: bool = False
):
    """
    3-PHASE PIPELINE:
    
    Phase 1: Cut silent parts (auto-editor)
    Phase 2: Enhance audio (DeepFilterNet AI)
    Phase 3: Cut awkward parts / fillers (transcribe ‚Üí detect ‚Üí cut)
    """
    
    if not input_path.exists():
        error(f"File not found: {input_path}")
        sys.exit(1)
    
    original_duration = get_duration(input_path)
    if original_duration <= 0:
        error("Could not determine video duration")
        sys.exit(1)
    
    # Header
    print()
    cprint("‚ïê" * 65, Colors.CYAN)
    cprint("  üé¨ ENHANCED AUTO-EDITOR", Colors.BOLD)
    cprint("‚ïê" * 65, Colors.CYAN)
    info(f"Input: {input_path.name}")
    info(f"Duration: {format_duration(original_duration)}")
    info(f"Transcription: {'‚òÅÔ∏è  Groq API (Whisper Large V3)' if use_groq else 'üñ•Ô∏è  Local Whisper'}")
    info(f"Enhancement: {'üß† DeepFilterNet AI' if enhance else '‚ùå Disabled'}")
    print()
    
    # Check Groq API key
    api_key = None
    if use_groq:
        api_key = get_groq_key()
        if not api_key:
            error("No Groq API key found!")
            info("Set it with: edit --set-groq-key YOUR_KEY")
            info("Get free key: https://console.groq.com/keys")
            sys.exit(1)
    
    base_name = input_path.stem
    work_dir = input_path.parent
    
    # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    # ‚ïë  PHASE 1: CUT SILENT PARTS                                    ‚ïë
    # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    print()
    cprint("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê", Colors.BLUE)
    cprint("‚îÇ  PHASE 1: CUT SILENT PARTS                              ‚îÇ", Colors.BLUE + Colors.BOLD)
    cprint("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò", Colors.BLUE)
    
    silence_removed_file = work_dir / f"{base_name}_no_silence.mp4"
    
    if silence_removed_file.exists() and silence_removed_file.stat().st_size > 10000:
        info("Using cached file...")
        phase1_duration = get_duration(silence_removed_file)
        silence_saved = original_duration - phase1_duration
        silence_saved_pct = (silence_saved / original_duration * 100) if original_duration > 0 else 0
        success(f"Already cut: saved {format_duration(silence_saved)} ({silence_saved_pct:.0f}%)")
    else:
        silence_removed_file, phase1_duration = phase1_cut_silence(input_path, silence_removed_file)
    
    # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    # ‚ïë  PHASE 2: ENHANCE AUDIO (DeepFilterNet AI)                    ‚ïë
    # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    processing_file = silence_removed_file
    if enhance:
        print()
        cprint("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê", Colors.BLUE)
        cprint("‚îÇ  PHASE 2: ENHANCE AUDIO (DeepFilterNet AI)              ‚îÇ", Colors.BLUE + Colors.BOLD)
        cprint("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò", Colors.BLUE)
        
        enhanced_file = work_dir / f"{base_name}_enhanced.mp4"
        
        if enhanced_file.exists() and enhanced_file.stat().st_size > 10000:
            info("Using cached enhanced file...")
            processing_file = enhanced_file
        else:
            processing_file = phase2_enhance_audio(silence_removed_file, enhanced_file)
    
    # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    # ‚ïë  PHASE 3: CUT AWKWARD PARTS (FILLERS)                         ‚ïë
    # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    print()
    cprint("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê", Colors.BLUE)
    cprint("‚îÇ  PHASE 3: CUT AWKWARD PARTS (FILLERS)                   ‚îÇ", Colors.BLUE + Colors.BOLD)
    cprint("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò", Colors.BLUE)
    
    output_xml = work_dir / f"{base_name}_premiere.xml"
    output_video = work_dir / f"{base_name}_final.mp4" if render else None
    
    words, xml_path, video_path = phase3_cut_fillers(
        processing_file,
        phase1_duration,
        use_groq,
        api_key,
        chunk_size,
        output_xml,
        output_video,
        render
    )
    
    # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    # ‚ïë  SUMMARY                                                      ‚ïë
    # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    print()
    cprint("‚ïê" * 65, Colors.GREEN)
    cprint("  ‚úÖ PROCESSING COMPLETE!", Colors.BOLD + Colors.GREEN)
    cprint("‚ïê" * 65, Colors.GREEN)
    print()
    
    if video_path and video_path.exists():
        final_duration = get_duration(video_path)
    else:
        filler_cuts = detect_filler_cuts(words)
        merged_cuts = merge_cuts(filler_cuts)
        filler_time = sum(e - s for s, e in merged_cuts)
        final_duration = phase1_duration - filler_time
    
    total_saved = original_duration - final_duration
    total_saved_pct = (total_saved / original_duration * 100) if original_duration > 0 else 0
    
    success(f"Original:  {format_duration(original_duration)}")
    success(f"Final:     {format_duration(final_duration)}")
    success(f"Saved:     {format_duration(total_saved)} ({total_saved_pct:.1f}%)")
    print()
    
    cprint("  üìÅ OUTPUT FILES:", Colors.BOLD)
    if xml_path.exists():
        cprint(f"     ‚Üí Premiere Pro: {xml_path.name}", Colors.CYAN)
    if video_path and video_path.exists():
        cprint(f"     ‚Üí Final Video:  {video_path.name}", Colors.CYAN)
    if enhance and processing_file != silence_removed_file:
        cprint(f"     ‚Üí Enhanced:     {processing_file.name}", Colors.CYAN)
    cprint(f"     ‚Üí No Silence:   {silence_removed_file.name}", Colors.CYAN)
    
    print()
    cprint("  üìù NEXT STEPS:", Colors.BOLD)
    cprint("     1. Import the XML into Premiere Pro", Colors.YELLOW)
    cprint("     2. Fine-tune cuts as needed", Colors.YELLOW)
    cprint("     3. Add music, transitions, etc.", Colors.YELLOW)
    cprint("‚ïê" * 65, Colors.GREEN)

def interactive_mode() -> Tuple[bool, int]:
    print()
    cprint("‚ïê" * 55, Colors.CYAN)
    cprint("  SELECT TRANSCRIPTION ENGINE", Colors.BOLD)
    cprint("‚ïê" * 55, Colors.CYAN)
    print()
    print("  [1] üñ•Ô∏è  Local Whisper (small model)")
    print("      ‚Ä¢ Works offline, uses your GPU")
    print("      ‚Ä¢ Less accurate for mixed Arabic/English")
    print()
    print("  [2] ‚òÅÔ∏è  Groq API (Whisper Large V3 Turbo) ‚≠ê RECOMMENDED")
    print("      ‚Ä¢ FREE: 7,200 audio sec/hour")
    print("      ‚Ä¢ Best accuracy for Arabic/English")
    print()
    
    api_key = get_groq_key()
    if api_key:
        cprint(f"  ‚úì Groq API key configured", Colors.GREEN)
    else:
        cprint(f"  ‚úó No Groq API key (get free at console.groq.com)", Colors.YELLOW)
    
    print()
    choice = input("  Choose [1/2] (default: 2): ").strip()
    
    use_groq = choice != "1"
    
    if use_groq and not api_key:
        print()
        cprint("  Get your FREE API key:", Colors.BOLD)
        cprint("  ‚Üí https://console.groq.com/keys", Colors.CYAN)
        print()
        key = input("  Enter Groq API key: ").strip()
        if key:
            set_groq_key(key)
        else:
            warn("No key provided, using local Whisper instead")
            use_groq = False
    
    return use_groq, GROQ_CHUNK_SIZE

def main():
    parser = argparse.ArgumentParser(
        description="Enhanced Auto-Editor - Remove silence, enhance audio, cut fillers",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
3-PHASE PIPELINE:
  Phase 1: Cut silent parts (auto-editor)
  Phase 2: Enhance audio (DeepFilterNet AI)
  Phase 3: Cut awkward parts / fillers ‚Üí Export XML

Examples:
  edit video.mp4                  # Interactive mode
  edit video.mp4 --groq           # Use Groq API
  edit video.mp4 --local          # Use local Whisper
  edit video.mp4 --render         # Also render final video
  edit video.mp4 --no-enhance     # Skip audio enhancement
  edit --set-groq-key             # Set your free API key

Get free Groq API key: https://console.groq.com/keys
        """
    )
    parser.add_argument("input_file", nargs="?", help="Path to video file")
    parser.add_argument("--local", action="store_true", help="Use local Whisper")
    parser.add_argument("--groq", action="store_true", help="Use Groq API (recommended)")
    parser.add_argument("--chunk", type=int, default=GROQ_CHUNK_SIZE, help=f"Chunk size for Groq (default: {GROQ_CHUNK_SIZE})")
    parser.add_argument("--set-groq-key", action="store_true", help="Set Groq API key")
    parser.add_argument("--no-enhance", action="store_true", help="Skip DeepFilterNet enhancement")
    parser.add_argument("--render", action="store_true", help="Render final video")
    
    args = parser.parse_args()
    
    if args.set_groq_key:
        print()
        cprint("  Get your FREE Groq API key:", Colors.BOLD)
        cprint("  ‚Üí https://console.groq.com/keys", Colors.CYAN)
        print()
        key = input("  Enter Groq API key: ").strip()
        if key:
            set_groq_key(key)
        else:
            error("No key provided")
        return
    
    if not args.input_file:
        parser.print_help()
        return
    
    input_path = Path(args.input_file).resolve()
    
    if args.groq:
        use_groq = True
        chunk_size = args.chunk
    elif args.local:
        use_groq = False
        chunk_size = GROQ_CHUNK_SIZE
    else:
        use_groq, chunk_size = interactive_mode()
    
    enhance = not args.no_enhance
    
    # Manage hyprwhspr to free up VRAM
    manage_hyprwhspr(stop=True)
    
    try:
        process_video(
            input_path,
            use_groq=use_groq,
            chunk_size=chunk_size,
            enhance=enhance,
            render=args.render
        )
    finally:
        # Always try to restart hyprwhspr
        manage_hyprwhspr(stop=False)

if __name__ == "__main__":
    main()
