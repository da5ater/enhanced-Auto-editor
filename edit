#!/usr/bin/env python3
"""
Enhanced Auto-Editor - Smart Video Editor

Removes silence, enhances audio (DeepFilterNet AI), and removes filler words.

3-PHASE PIPELINE:
  Phase 1: Cut silent parts (auto-editor, fast, local)
  Phase 2: Enhance audio with DeepFilterNet AI (studio-quality voice)
  Phase 3: Cut awkward parts / fillers (transcribe ‚Üí detect ‚Üí cut)

Supports:
  - Local Whisper (small model) for 4GB VRAM GPUs
  - Groq API (Whisper Large V3 Turbo) for better accuracy (FREE tier)
  - DeepFilterNet AI for professional voice enhancement

Usage:
  edit video.mp4                    # Interactive mode
  edit video.mp4 --local            # Force local Whisper small
  edit video.mp4 --groq             # Force Groq API (recommended)
  edit video.mp4 --set-groq-key     # Set/update Groq API key
  edit video.mp4 --no-enhance       # Skip audio enhancement
  edit video.mp4 --render           # Render final video (default: XML only)

Groq Free Tier Limits:
  - 7,200 audio seconds/hour (2 hours of audio per hour)
  - 28,800 audio seconds/day (8 hours of audio per day)
  - By removing silence FIRST, you use less API quota!
"""

import os
import sys
import subprocess
import json
import math
import argparse
import time
import tempfile
import shutil
import signal
import gc  # Added for memory cleanup
from pathlib import Path

# ... (imports remain the same) ...

def transcribe_local(audio_path: Path, duration: float) -> List[WordSegment]:
    """Transcribe with GPU-only fallback: medium ‚Üí small ‚Üí base ‚Üí tiny"""
    try:
        from faster_whisper import WhisperModel
    except ImportError:
        error("faster-whisper not installed! Run: pip install faster-whisper")
        sys.exit(1)
    
    prompt = "ŸäÿπŸÜŸä, ÿßŸÖŸÖ, ÿßÿß, ÿßŸá, ÿ∑ÿ®, ÿ®ŸÇŸâ, Um, uh, ah, like, so, you know."
    
    # GPU-only fallback sequence
    # STARTING FROM MEDIUM as requested
    models_to_try = [
        "medium",  # Requested start point (balanced)
        "small",
        "base",
        "tiny"
    ]
    
    for model_name in models_to_try:
        display_name = model_name.split("/")[-1]
        try:
            info(f"  Loading {display_name} model on GPU...")
            # Use float16 or int8_float16 for better VRAM usage on RTX 3050
            model = WhisperModel(model_name, device="cuda", compute_type="float16")
            success(f"  Loaded {display_name} on GPU")
            
            info("  Transcribing...")
            bar = ProgressBar(total=duration)
            
            segments_gen, _ = model.transcribe(
                str(audio_path),
                beam_size=5,
                word_timestamps=True,
                initial_prompt=prompt,
                language=None,
                vad_filter=True,
                vad_parameters=dict(min_silence_duration_ms=500)
            )
            
            words = []
            for segment in segments_gen:
                bar.update(segment.end, "Transcribing")
                if segment.words:
                    for w in segment.words:
                        words.append(WordSegment(
                            word=w.word.strip(),
                            start=w.start,
                            end=w.end,
                            confidence=math.exp(segment.avg_logprob)
                        ))
            
            bar.finish("Transcription complete")
            
            # Cleanup memory explicitly
            del model
            gc.collect()
            
            return words
            
        except Exception as e:
            warn(f"  {display_name} failed: {str(e)[:100]}...")
            
            # Explicit cleanup on failure
            if 'model' in locals():
                del model
            gc.collect()
            # torch.cuda.empty_cache() # If torch was used directly
            
            if model_name != models_to_try[-1]:
                info(f"  Trying smaller model...")
            continue
    
    error("All GPU models failed! Try using --groq instead.")
    sys.exit(1)

def transcribe_groq_chunk(audio_path: Path, api_key: str, offset: float = 0) -> List[WordSegment]:
    try:
        from groq import Groq
    except ImportError:
        error("groq SDK not installed! Run: pip install groq")
        sys.exit(1)
    
    client = Groq(api_key=api_key)
    
    with open(audio_path, "rb") as f:
        response = client.audio.transcriptions.create(
            file=f,
            model=GROQ_MODEL,
            response_format="verbose_json",
            timestamp_granularities=["word"],
            language="ar",
            prompt="ŸäÿπŸÜŸä, ÿßŸÖŸÖ, ÿßÿß, ÿßŸá, ÿ∑ÿ®, ÿ®ŸÇŸâ, Um, uh, ah, like, so, you know."
        )
    
    words = []
    if hasattr(response, 'words') and response.words:
        for w in response.words:
            words.append(WordSegment(
                word=w.word if hasattr(w, 'word') else w.get('word', ''),
                start=(w.start if hasattr(w, 'start') else w.get('start', 0)) + offset,
                end=(w.end if hasattr(w, 'end') else w.get('end', 0)) + offset,
                confidence=0.9
            ))
    elif hasattr(response, 'segments') and response.segments:
        for seg in response.segments:
            if hasattr(seg, 'words') and seg.words:
                for w in seg.words:
                    words.append(WordSegment(
                        word=w.word if hasattr(w, 'word') else w.get('word', ''),
                        start=(w.start if hasattr(w, 'start') else w.get('start', 0)) + offset,
                        end=(w.end if hasattr(w, 'end') else w.get('end', 0)) + offset,
                        confidence=0.9
                    ))
    
    return words

def transcribe_groq(input_path: Path, duration: float, api_key: str, chunk_size: int) -> List[WordSegment]:
    info(f"  Using Groq API (Whisper Large V3 Turbo)...")
    
    chunks = []
    start = 0
    while start < duration:
        end = min(start + chunk_size, duration)
        chunks.append((start, end - start))
        start = end - GROQ_OVERLAP if end < duration else end
    
    info(f"  Processing {len(chunks)} chunks...")
    bar = ProgressBar(total=len(chunks))
    
    all_words = []
    
    with tempfile.TemporaryDirectory() as tmpdir:
        tmpdir = Path(tmpdir)
        
        for i, (start_time, chunk_duration) in enumerate(chunks):
            chunk_path = tmpdir / f"chunk_{i:04d}.flac"
            extract_audio_chunk(input_path, chunk_path, start_time, chunk_duration)
            
            size_mb = chunk_path.stat().st_size / (1024 * 1024)
            if size_mb > 24:
                warn(f"  Chunk {i} is {size_mb:.1f}MB (>24MB), may fail.")
            
            try:
                words = transcribe_groq_chunk(chunk_path, api_key, offset=start_time)
                all_words.extend(words)
            except Exception as e:
                warn(f"  Chunk {i} failed: {e}")
            
            bar.update(i + 1, f"Chunk {i+1}/{len(chunks)}")
            
            if i < len(chunks) - 1:
                time.sleep(0.5)
    
    bar.finish("Transcription complete")
    
    all_words.sort(key=lambda w: w.start)
    deduped = []
    for w in all_words:
        if not deduped or w.start >= deduped[-1].end - 0.1:
            deduped.append(w)
    
    return deduped

# --- Filler Detection ---
def detect_filler_cuts(words: List[WordSegment]) -> List[Tuple[float, float]]:
    cuts = []
    
    for w in words:
        clean = w.word.lower().strip().replace(",", "").replace(".", "").replace("?", "").replace("ÿå", "")
        duration = w.end - w.start
        
        if duration < FILLER_MIN_DURATION or duration > FILLER_MAX_DURATION:
            continue
        
        is_filler = False
        
        if clean in FILLERS:
            is_filler = True
        
        if len(clean) > 2 and len(set(clean)) == 1:
            is_filler = True
        
        if w.confidence < MIN_CONFIDENCE and duration < 1.0:
            is_filler = True
        
        if is_filler:
            cuts.append((max(0, w.start - 0.01), w.end + 0.01))
    
    return cuts

def merge_cuts(cuts: List[Tuple[float, float]], merge_gap: float = 0.1) -> List[Tuple[float, float]]:
    if not cuts:
        return []
    
    cuts = sorted(cuts, key=lambda x: x[0])
    merged = [cuts[0]]
    
    for start, end in cuts[1:]:
        last_start, last_end = merged[-1]
        if start <= last_end + merge_gap:
            merged[-1] = (last_start, max(last_end, end))
        else:
            merged.append((start, end))
    
    return merged

# === 3-PHASE PIPELINE ===

def phase1_cut_silence(input_path: Path, output_path: Path) -> Tuple[Path, float]:
    """
    PHASE 1: Cut silent parts
    Uses auto-editor for fast, local silence detection and removal.
    """
    info("Cutting silent parts...")
    
    ae_path = find_auto_editor()
    original_duration = get_duration(input_path)
    
    cmd = [
        ae_path,
        str(input_path),
        "--edit", "audio:threshold=3%,minclip=0.25s,mincut=0.15s",
        "--margin", "0.12s,0.18s",
        "--video-codec", "h264_nvenc",
        "--video-bitrate", "10M",
        "--audio-codec", "aac",
        "--audio-bitrate", "192k",
        "--output", str(output_path),
        "--progress", "machine",
    ]
    
    try:
        subprocess.run(cmd, capture_output=True, text=True, check=True)
        new_duration = get_duration(output_path)
        saved = original_duration - new_duration
        saved_pct = (saved / original_duration * 100) if original_duration > 0 else 0
        success(f"Silence cut: {format_duration(original_duration)} ‚Üí {format_duration(new_duration)} (-{saved_pct:.1f}%)")
        return output_path, new_duration
    except subprocess.CalledProcessError as e:
        error(f"auto-editor failed: {e.stderr[:500] if e.stderr else 'Unknown error'}")
        sys.exit(1)

def phase2_enhance_audio(input_path: Path, output_path: Path) -> Path:
    """
    PHASE 2: Enhance audio with DeepFilterNet AI
    Removes background noise, enhances voice clarity.
    """
    if enhance_audio_deepfilter(input_path, output_path):
        return output_path
    else:
        warn("DeepFilterNet failed, using original audio")
        return input_path

def generate_filler_report(words: List[WordSegment], cuts: List[Tuple[float, float]], output_path: Path):
    """Generates a readable report of what was detected and cut"""
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("=== FILLER WORD REPORT ===\n")
        f.write(f"Total Cuts: {len(cuts)}\n")
        f.write(f"Total Time Cut: {sum(e-s for s,e in cuts):.2f} seconds\n\n")
        
        f.write("--- DETAILED CUTS ---\n")
        
        # Map cuts back to words for context
        cut_idx = 0
        for w in words:
            if cut_idx < len(cuts):
                cut_start, cut_end = cuts[cut_idx]
                # If word is inside the current cut
                if w.start >= cut_start - 0.1 and w.end <= cut_end + 0.1:
                    time_str = format_duration(w.start)
                    f.write(f"[{time_str}] {w.word} ({w.confidence:.2f})\n")
                    
                # Move to next cut if we passed this one
                if w.end > cut_end:
                    cut_idx += 1

def phase3_cut_fillers(
    input_path: Path,
    duration: float,
    use_groq: bool,
    api_key: Optional[str],
    chunk_size: int,
    output_xml: Path,
    output_video: Optional[Path] = None,
    render: bool = False
) -> Tuple[List[WordSegment], Path, Optional[Path]]:
    """
    PHASE 3: Cut awkward parts (filler words)
    1. Transcribe audio to get word timestamps
    2. Detect filler words (um, ah, ŸäÿπŸÜŸä, etc.)
    3. Export Premiere Pro XML with cuts marked
    """
    
    # Step 1: Transcribe
    info("Transcribing audio...")
    if use_groq:
        if not api_key:
            error("No Groq API key!")
            sys.exit(1)
        audio_minutes = duration / 60
        info(f"  Audio: {format_duration(duration)} ({audio_minutes:.1f} min)")
        words = transcribe_groq(input_path, duration, api_key, chunk_size)
    else:
        temp_wav = input_path.parent / f"{input_path.stem}_temp.wav"
        try:
            info("  Extracting audio...")
            extract_audio_wav(input_path, temp_wav)
            words = transcribe_local(temp_wav, duration)
        finally:
            if temp_wav.exists():
                temp_wav.unlink()
    
    info(f"  Found {len(words)} words")
    
    # Step 2: Detect fillers
    info("Detecting filler words...")
    filler_cuts = detect_filler_cuts(words)
    merged_cuts = merge_cuts(filler_cuts)
    total_filler_time = sum(e - s for s, e in merged_cuts)
    info(f"  Found {len(merged_cuts)} fillers ({total_filler_time:.1f}s)")
    
    # Generate Report
    report_path = input_path.parent / f"{input_path.stem}_fillers.txt"
    generate_filler_report(words, merged_cuts, report_path)
    success(f"Filler report saved to: {report_path.name}")
    
    # Step 3: Export
    ae_path = find_auto_editor()
    
    cut_args = []
    if merged_cuts:
        ranges = [f"{s:.3f}s,{e:.3f}s" for s, e in merged_cuts]
        cut_args = ["--cut-out"] + ranges
    
    info("Exporting Premiere Pro XML...")
    
    # Check for argument length limit (simple heuristic)
    if len(cut_args) > 1000:
        warn(f"Too many cuts ({len(merged_cuts)})! Splitting export or reducing precision might be needed.")
        # We proceed anyway, but warn user
    
    cmd_xml = [
        ae_path,
        str(input_path),
        "--edit", "none",
    ] + cut_args + [
        "--export", "premiere",
        "--output", str(output_xml),
        "--progress", "machine",
    ]
    
    try:
        subprocess.run(cmd_xml, capture_output=True, text=True, check=True)
        success(f"XML exported: {output_xml.name}")
    except subprocess.CalledProcessError as e:
        warn(f"XML export failed: {e.stderr[:300] if e.stderr else 'Unknown'}")
        warn("But don't worry! The list of cuts is saved in 'fillers.txt'.")
    
    # Optional: Render final video
    rendered_path = None
    if render and output_video:
        info("Rendering final video...")
        cmd_render = [
            ae_path,
            str(input_path),
            "--edit", "none",
        ] + cut_args + [
            "--video-codec", "h264_nvenc",
            "--video-bitrate", "8M",
            "--audio-codec", "aac",
            "--audio-bitrate", "192k",
            "--output", str(output_video),
            "--progress", "machine",
        ]
        
        try:
            subprocess.run(cmd_render, capture_output=True, text=True, check=True)
            success(f"Video rendered: {output_video.name}")
            rendered_path = output_video
        except subprocess.CalledProcessError as e:
            warn(f"Render failed: {e.stderr[:300] if e.stderr else 'Unknown'}")
    
    return words, output_xml, rendered_path

# --- Main Processing Pipeline ---
def process_video(
    input_path: Path,
    use_groq: bool = False,
    chunk_size: int = GROQ_CHUNK_SIZE,
    enhance: bool = True,
    render: bool = False
):
    """
    3-PHASE PIPELINE:
    
    Phase 1: Cut silent parts (auto-editor)
    Phase 2: Enhance audio (DeepFilterNet AI)
    Phase 3: Cut awkward parts / fillers (transcribe ‚Üí detect ‚Üí cut)
    """
    
    if not input_path.exists():
        error(f"File not found: {input_path}")
        sys.exit(1)
    
    original_duration = get_duration(input_path)
    if original_duration <= 0:
        error("Could not determine video duration")
        sys.exit(1)
    
    # Header
    print()
    cprint("‚ïê" * 65, Colors.CYAN)
    cprint("  üé¨ ENHANCED AUTO-EDITOR", Colors.BOLD)
    cprint("‚ïê" * 65, Colors.CYAN)
    info(f"Input: {input_path.name}")
    info(f"Duration: {format_duration(original_duration)}")
    info(f"Transcription: {'‚òÅÔ∏è  Groq API (Whisper Large V3)' if use_groq else 'üñ•Ô∏è  Local Whisper'}")
    info(f"Enhancement: {'üß† DeepFilterNet AI' if enhance else '‚ùå Disabled'}")
    print()
    
    # Check Groq API key
    api_key = None
    if use_groq:
        api_key = get_groq_key()
        if not api_key:
            error("No Groq API key found!")
            info("Set it with: edit --set-groq-key YOUR_KEY")
            info("Get free key: https://console.groq.com/keys")
            sys.exit(1)
    
    base_name = input_path.stem
    work_dir = input_path.parent
    
    # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    # ‚ïë  PHASE 1: CUT SILENT PARTS                                    ‚ïë
    # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    print()
    cprint("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê", Colors.BLUE)
    cprint("‚îÇ  PHASE 1: CUT SILENT PARTS                              ‚îÇ", Colors.BLUE + Colors.BOLD)
    cprint("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò", Colors.BLUE)
    
    silence_removed_file = work_dir / f"{base_name}_no_silence.mp4"
    
    if silence_removed_file.exists() and silence_removed_file.stat().st_size > 10000:
        info("Using cached file...")
        phase1_duration = get_duration(silence_removed_file)
        silence_saved = original_duration - phase1_duration
        silence_saved_pct = (silence_saved / original_duration * 100) if original_duration > 0 else 0
        success(f"Already cut: saved {format_duration(silence_saved)} ({silence_saved_pct:.0f}%)")
    else:
        silence_removed_file, phase1_duration = phase1_cut_silence(input_path, silence_removed_file)
    
    # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    # ‚ïë  PHASE 2: ENHANCE AUDIO (DeepFilterNet AI)                    ‚ïë
    # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    processing_file = silence_removed_file
    if enhance:
        print()
        cprint("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê", Colors.BLUE)
        cprint("‚îÇ  PHASE 2: ENHANCE AUDIO (DeepFilterNet AI)              ‚îÇ", Colors.BLUE + Colors.BOLD)
        cprint("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò", Colors.BLUE)
        
        enhanced_file = work_dir / f"{base_name}_enhanced.mp4"
        
        if enhanced_file.exists() and enhanced_file.stat().st_size > 10000:
            info("Using cached enhanced file...")
            processing_file = enhanced_file
        else:
            processing_file = phase2_enhance_audio(silence_removed_file, enhanced_file)
    
    # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    # ‚ïë  PHASE 3: CUT AWKWARD PARTS (FILLERS)                         ‚ïë
    # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    print()
    cprint("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê", Colors.BLUE)
    cprint("‚îÇ  PHASE 3: CUT AWKWARD PARTS (FILLERS)                   ‚îÇ", Colors.BLUE + Colors.BOLD)
    cprint("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò", Colors.BLUE)
    
    output_xml = work_dir / f"{base_name}_premiere.xml"
    output_video = work_dir / f"{base_name}_final.mp4" if render else None
    
    words, xml_path, video_path = phase3_cut_fillers(
        processing_file,
        phase1_duration,
        use_groq,
        api_key,
        chunk_size,
        output_xml,
        output_video,
        render
    )
    
    # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    # ‚ïë  SUMMARY                                                      ‚ïë
    # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    print()
    cprint("‚ïê" * 65, Colors.GREEN)
    cprint("  ‚úÖ PROCESSING COMPLETE!", Colors.BOLD + Colors.GREEN)
    cprint("‚ïê" * 65, Colors.GREEN)
    print()
    
    if video_path and video_path.exists():
        final_duration = get_duration(video_path)
    else:
        filler_cuts = detect_filler_cuts(words)
        merged_cuts = merge_cuts(filler_cuts)
        filler_time = sum(e - s for s, e in merged_cuts)
        final_duration = phase1_duration - filler_time
    
    total_saved = original_duration - final_duration
    total_saved_pct = (total_saved / original_duration * 100) if original_duration > 0 else 0
    
    success(f"Original:  {format_duration(original_duration)}")
    success(f"Final:     {format_duration(final_duration)}")
    success(f"Saved:     {format_duration(total_saved)} ({total_saved_pct:.1f}%)")
    print()
    
    cprint("  üìÅ OUTPUT FILES:", Colors.BOLD)
    if xml_path.exists():
        cprint(f"     ‚Üí Premiere Pro: {xml_path.name}", Colors.CYAN)
    if video_path and video_path.exists():
        cprint(f"     ‚Üí Final Video:  {video_path.name}", Colors.CYAN)
    if enhance and processing_file != silence_removed_file:
        cprint(f"     ‚Üí Enhanced:     {processing_file.name}", Colors.CYAN)
    cprint(f"     ‚Üí No Silence:   {silence_removed_file.name}", Colors.CYAN)
    
    print()
    cprint("  üìù NEXT STEPS:", Colors.BOLD)
    cprint("     1. Import the XML into Premiere Pro", Colors.YELLOW)
    cprint("     2. Fine-tune cuts as needed", Colors.YELLOW)
    cprint("     3. Add music, transitions, etc.", Colors.YELLOW)
    cprint("‚ïê" * 65, Colors.GREEN)

def interactive_mode() -> Tuple[bool, int]:
    print()
    cprint("‚ïê" * 55, Colors.CYAN)
    cprint("  SELECT TRANSCRIPTION ENGINE", Colors.BOLD)
    cprint("‚ïê" * 55, Colors.CYAN)
    print()
    print("  [1] üñ•Ô∏è  Local Whisper (small model)")
    print("      ‚Ä¢ Works offline, uses your GPU")
    print("      ‚Ä¢ Less accurate for mixed Arabic/English")
    print()
    print("  [2] ‚òÅÔ∏è  Groq API (Whisper Large V3 Turbo) ‚≠ê RECOMMENDED")
    print("      ‚Ä¢ FREE: 7,200 audio sec/hour")
    print("      ‚Ä¢ Best accuracy for Arabic/English")
    print()
    
    api_key = get_groq_key()
    if api_key:
        cprint(f"  ‚úì Groq API key configured", Colors.GREEN)
    else:
        cprint(f"  ‚úó No Groq API key (get free at console.groq.com)", Colors.YELLOW)
    
    print()
    choice = input("  Choose [1/2] (default: 2): ").strip()
    
    use_groq = choice != "1"
    
    if use_groq and not api_key:
        print()
        cprint("  Get your FREE API key:", Colors.BOLD)
        cprint("  ‚Üí https://console.groq.com/keys", Colors.CYAN)
        print()
        key = input("  Enter Groq API key: ").strip()
        if key:
            set_groq_key(key)
        else:
            warn("No key provided, using local Whisper instead")
            use_groq = False
    
    return use_groq, GROQ_CHUNK_SIZE

def main():
    parser = argparse.ArgumentParser(
        description="Enhanced Auto-Editor - Remove silence, enhance audio, cut fillers",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
3-PHASE PIPELINE:
  Phase 1: Cut silent parts (auto-editor)
  Phase 2: Enhance audio (DeepFilterNet AI)
  Phase 3: Cut awkward parts / fillers ‚Üí Export XML

Examples:
  edit video.mp4                  # Interactive mode
  edit video.mp4 --groq           # Use Groq API
  edit video.mp4 --local          # Use local Whisper
  edit video.mp4 --render         # Also render final video
  edit video.mp4 --no-enhance     # Skip audio enhancement
  edit --set-groq-key             # Set your free API key

Get free Groq API key: https://console.groq.com/keys
        """
    )
    parser.add_argument("input_file", nargs="?", help="Path to video file")
    parser.add_argument("--local", action="store_true", help="Use local Whisper")
    parser.add_argument("--groq", action="store_true", help="Use Groq API (recommended)")
    parser.add_argument("--chunk", type=int, default=GROQ_CHUNK_SIZE, help=f"Chunk size for Groq (default: {GROQ_CHUNK_SIZE})")
    parser.add_argument("--set-groq-key", action="store_true", help="Set Groq API key")
    parser.add_argument("--no-enhance", action="store_true", help="Skip DeepFilterNet enhancement")
    parser.add_argument("--render", action="store_true", help="Render final video")
    
    args = parser.parse_args()
    
    if args.set_groq_key:
        print()
        cprint("  Get your FREE Groq API key:", Colors.BOLD)
        cprint("  ‚Üí https://console.groq.com/keys", Colors.CYAN)
        print()
        key = input("  Enter Groq API key: ").strip()
        if key:
            set_groq_key(key)
        else:
            error("No key provided")
        return
    
    if not args.input_file:
        parser.print_help()
        return
    
    input_path = Path(args.input_file).resolve()
    
    if args.groq:
        use_groq = True
        chunk_size = args.chunk
    elif args.local:
        use_groq = False
        chunk_size = GROQ_CHUNK_SIZE
    else:
        use_groq, chunk_size = interactive_mode()
    
    enhance = not args.no_enhance
    
    # Manage hyprwhspr to free up VRAM
    manage_hyprwhspr(stop=True)
    
    try:
        process_video(
            input_path,
            use_groq=use_groq,
            chunk_size=chunk_size,
            enhance=enhance,
            render=args.render
        )
    finally:
        # Always try to restart hyprwhspr
        manage_hyprwhspr(stop=False)

if __name__ == "__main__":
    main()
